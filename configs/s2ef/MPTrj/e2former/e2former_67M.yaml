includes:
  - configs/s2ef/MPTrj/base.yml

trainer: new_equiformerv2_forces
print_every: 200

dataset:
  train:
    format: lmdb
    src: data/mptrj/out/train
    key_mapping:
      corrected_total_energy: energy
      force: forces
    transforms:
      normalizer:
        energy:
          mean: -201.6314239501953
          stdev: 203.66830444335938
        forces:
          mean: 0.0
          stdev: 0.807562530040741
  val:
    src: data/mptrj/out/val

logger: wandb

outputs:
  energy:
    shape: 1
    level: system
    prediction_dtype: float32
  forces:
    irrep_dim: 1
    level: atom
    train_on_free_atoms: True
    eval_on_free_atoms: True
    prediction_dtype: float32

loss_functions:
  - energy:
      fn: mae
      coefficient: 4
  - forces:
      fn: l2mae
      coefficient: 100

evaluation_metrics:
  metrics:
    energy:
      - mae
    forces:
      - mae
      - cosine_similarity
      - magnitude_error
    misc:
      - energy_forces_within_threshold
  primary_metric: energy_mae

model:
  name: hydra
  pass_through_head_outputs: True
  otf_graph: True

  backbone:
    model: src.models.E2Former_wrapper.E2FormerBackbone
    encoder: "default" # or dit

    # dit_config
    encoder_embed_dim: 256
    ffn_embedding_dim: 1536
    num_attention_heads: 32
    dropout: 0.1
    num_encoder_layers: 12


    # global_cfg:
    regress_forces: true
    direct_force: false
    use_padding: true
    use_fp16_backbone: false
    use_compile: false
    hidden_size: 256
    # batch_size: 12
    # activation: gelu

  #  # molecular_graph_cfg:
    use_pbc: true
    use_pbc_single: false
    otf_graph: true
  #   max_num_elements: 90
  #   max_num_nodes_per_batch: 150 # Average 73, Max 220, use 150 for padding
  #   enforce_max_neighbors_strictly: false
  #   distance_function: gaussian

  # # gnn_cfg:
  #   atom_embedding_size: 128
  #   node_direction_embedding_size: 64
  #   node_direction_expansion_size: 64
  #   edge_distance_expansion_size: 128
  #   edge_distance_embedding_size: 64
  #   atten_name: flash
  #   atten_num_heads: 4
  #   readout_hidden_layer_multiplier: 2
  #   output_hidden_layer_multiplier: 2
  #   ffn_hidden_layer_multiplier: 4
  #   use_angle_embedding: true

  #   # reg_cfg:
  #   mlp_dropout: 0.1
  #   atten_dropout: 0.1
  #   stochastic_depth_prob: 0.1
  #   normalization: layernorm

    # psm_config:
    pbc_expanded_token_cutoff: 512
    pbc_expanded_num_cell_per_direction: 4

  # backbone_config:
    decouple_EF: True
    max_neighbors: 20
    irreps_node_embedding: "256x0e+256x1e+256x2e+256x3e"
    num_layers: 12
    pbc_max_radius: 12
    max_radius: 12
    basis_type: "gaussiansmear"
    number_of_basis: 256
    num_attn_heads: 32
    attn_scalar_head: 16   # same as the irreps head
    irreps_head: "16x0e+16x1e+16x2e+16x3e"
    rescale_degree: False
    nonlinear_message: False
    norm_layer: "layer_norm_sh"
    alpha_drop: 0.05
    proj_drop: 0.0
    out_drop: 0.0
    drop_path_rate: 0.05
    attn_type: "all-order"
    tp_type : 'dot_alpha'
    edge_embedtype: "highorder"
    attn_biastype: "share"
    ffn_type: 's2'
    add_rope: False
    time_embed: False
    force_head: null

  heads:
    forces:
      module: src.models.E2Former_wrapper.E2FormerEasyForceHead
    energy:
      module: src.models.E2Former_wrapper.E2FormerEasyEnergyHead

optim:
  batch_size:               28
  eval_batch_size:          4
  load_balancing:           atoms
  num_workers:              8
  lr_initial:               0.0002

  optimizer:                AdamW
  optimizer_params:
    weight_decay:           0.001
  scheduler:                LambdaLR
  scheduler_params:
    lambda_type:            cosine
    warmup_factor:          0.2
    warmup_epochs:          0.1
    lr_min_factor:          0.01

  max_epochs:               10
  clip_grad_norm:           50
  ema_decay:                0.999

  eval_every:               5000
